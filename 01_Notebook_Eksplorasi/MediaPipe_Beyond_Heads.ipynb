{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f0dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "```json\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# MediaPipe Beyond Hand Tracking - Eksplorasi Komprehensif\\n\",\n",
    "    \"**Tugas Besar Pengolahan Citra Digital**  \\n\",\n",
    "    \"**Tim:** Rindi Indriani, Acit, Dian  \\n\",\n",
    "    \"**Tanggal:** 14 Juni 2025  \\n\",\n",
    "    \"**PIC Notebook:** Rindi Indriani\\n\",\n",
    "    \"\\n\",\n",
    "    \"## 🎯 Tujuan Eksplorasi\\n\",\n",
    "    \"Berdasarkan aplikasi baseline yang menggunakan **HEAD GESTURE CONTROL** (Face Mesh untuk deteksi tilt kepala), eksplorasi ini bertujuan:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Menganalisis performa baseline **Head Gesture Control** (Face Mesh current system)\\n\",\n",
    "    \"2. Mengeksplorasi **MediaPipe Pose** untuk full body tracking\\n\",\n",
    "    \"3. Menguji **Enhanced Face Features** (blink, smile, emotion detection)\\n\",\n",
    "    \"4. Menganalisis **MediaPipe Holistic** (face + pose + hands combined)\\n\",\n",
    "    \"5. Evaluasi performa dan akurasi pada berbagai kondisi\\n\",\n",
    "    \"6. Perbandingan dengan baseline head gesture system Acit\\n\",\n",
    "    \"7. Rekomendasi pengembangan \\\"beyond head tracking\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"## 📋 Baseline Project Analysis\\n\",\n",
    "    \"**Current System (by Acit):**\\n\",\n",
    "    \"- **Technology:** MediaPipe Face Mesh\\n\",\n",
    "    \"- **Function:** Head tilt detection untuk PowerPoint control\\n\",\n",
    "    \"- **Gestures:** \\n\",\n",
    "    \"  - Tilt Right (15°): Next slide\\n\",\n",
    "    \"  - Tilt Left (15°): Previous slide  \\n\",\n",
    "    \"  - Triple Tilt (20°): Close presentation\\n\",\n",
    "    \"- **Implementation:** 468 facial landmarks, head pose calculation based on eye line angle\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 📚 1. Setup dan Import Libraries\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Install required packages\\n\",\n",
    "    \"!pip install mediapipe opencv-python matplotlib seaborn pandas numpy plotly\\n\",\n",
    "    \"\\n\",\n",
    "    \"import cv2\\n\",\n",
    "    \"import mediapipe as mp\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import time\\n\",\n",
    "    \"import math\\n\",\n",
    "    \"import plotly.graph_objects as go\\n\",\n",
    "    \"import plotly.express as px\\n\",\n",
    "    \"from plotly.subplots import make_subplots\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set plotting style\\n\",\n",
    "    \"plt.style.use('seaborn-v0_8')\\n\",\n",
    "    \"sns.set_palette(\\\"husl\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"✅ All libraries imported successfully!\\\")\\n\",\n",
    "    \"print(f\\\"MediaPipe version: {mp.__version__}\\\")\\n\",\n",
    "    \"print(f\\\"OpenCV version: {cv2.__version__}\\\")\\n\",\n",
    "    \"print(\\\"\\\\n🎯 Project Context: Eksplorasi MediaPipe BEYOND current Head Gesture system\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 🔍 2. Baseline Analysis - Current Head Gesture System (Acit's Implementation)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Initialize MediaPipe Face Mesh (replicating Acit's baseline system)\\n\",\n",
    "    \"mp_face_mesh = mp.solutions.face_mesh\\n\",\n",
    "    \"mp_drawing = mp.solutions.drawing_utils\\n\",\n",
    "    \"mp_drawing_styles = mp.solutions.drawing_styles\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Performance tracking\\n\",\n",
    "    \"performance_data = {\\n\",\n",
    "    \"    'method': [],\\n\",\n",
    "    \"    'fps': [],\\n\",\n",
    "    \"    'detection_confidence': [],\\n\",\n",
    "    \"    'processing_time_ms': [],\\n\",\n",
    "    \"    'landmarks_count': [],\\n\",\n",
    "    \"    'accuracy_rate': [],\\n\",\n",
    "    \"    'use_case': []\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"def calculate_head_pose_acit_style(landmarks, image_size):\\n\",\n",
    "    \"    \\\"\\\"\\\"Replicate Acit's head pose calculation method\\\"\\\"\\\"\\n\",\n",
    "    \"    # Key facial landmarks for head pose estimation (same as Acit's code)\\n\",\n",
    "    \"    nose_tip = landmarks[1]\\n\",\n",
    "    \"    chin = landmarks[18]\\n\",\n",
    "    \"    left_eye_corner = landmarks[33]\\n\",\n",
    "    \"    right_eye_corner = landmarks[263]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Convert normalized coordinates to pixel coordinates\\n\",\n",
    "    \"    h, w = image_size\\n\",\n",
    "    \"    nose_tip = (int(nose_tip.x * w), int(nose_tip.y * h))\\n\",\n",
    "    \"    chin = (int(chin.x * w), int(chin.y * h))\\n\",\n",
    "    \"    left_eye = (int(left_eye_corner.x * w), int(left_eye_corner.y * h))\\n\",\n",
    "    \"    right_eye = (int(right_eye_corner.x * w), int(right_eye_corner.y * h))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate head tilt (roll) - based on eye line angle (Acit's method)\\n\",\n",
    "    \"    eye_center_x = (left_eye[0] + right_eye[0]) / 2\\n\",\n",
    "    \"    eye_center_y = (left_eye[1] + right_eye[1]) / 2\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate angle of eye line\\n\",\n",
    "    \"    dx = right_eye[0] - left_eye[0]\\n\",\n",
    "    \"    dy = right_eye[1] - left_eye[1]\\n\",\n",
    "    \"    roll_angle = math.degrees(math.atan2(dy, dx))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        'roll': roll_angle,\\n\",\n",
    "    \"        'nose_tip': nose_tip,\\n\",\n",
    "    \"        'chin': chin,\\n\",\n",
    "    \"        'left_eye': left_eye,\\n\",\n",
    "    \"        'right_eye': right_eye,\\n\",\n",
    "    \"        'eye_center': (int(eye_center_x), int(eye_center_y))\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"def detect_head_gestures_acit_style(head_pose):\\n\",\n",
    "    \"    \\\"\\\"\\\"Detect head gestures using Acit's thresholds\\\"\\\"\\\"\\n\",\n",
    "    \"    roll = head_pose['roll']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Acit's thresholds\\n\",\n",
    "    \"    tilt_threshold = 15  # degrees for navigation\\n\",\n",
    "    \"    triple_tilt_threshold = 20  # degrees for exit\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if roll > tilt_threshold:\\n\",\n",
    "    \"        return \\\"tilt_right\\\", roll\\n\",\n",
    "    \"    elif roll < -tilt_threshold:\\n\",\n",
    "    \"        return \\\"tilt_left\\\", roll\\n\",\n",
    "    \"    elif abs(roll) > triple_tilt_threshold:\\n\",\n",
    "    \"        return \\\"triple_tilt_candidate\\\", roll\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        return None, roll\\n\",\n",
    "    \"\\n\",\n",
    "    \"def test_baseline_head_gesture_system():\\n\",\n",
    "    \"    \\\"\\\"\\\"Test baseline head gesture system (Acit's implementation analysis)\\\"\\\"\\\"\\n\",\n",
    "    \"    cap = cv2.VideoCapture(0)\\n\",\n",
    "    \"    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)  # Same as Acit's setting\\n\",\n",
    "    \"    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    with mp_face_mesh.FaceMesh(\\n\",\n",
    "    \"        max_num_faces=1,\\n\",\n",
    "    \"        refine_landmarks=True,\\n\",\n",
    "    \"        min_detection_confidence=0.5,\\n\",\n",
    "    \"        min_tracking_confidence=0.5\\n\",\n",
    "    \"    ) as face_mesh:\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        frame_count = 0\\n\",\n",
    "    \"        start_time = time.time()\\n\",\n",
    "    \"        fps_list = []\\n\",\n",
    "    \"        processing_times = []\\n\",\n",
    "    \"        gesture_accuracy_data = []\\n\",\n",
    "    \"        head_detected_frames = 0\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(\\\"🎥 Testing Baseline Head Gesture System (Acit's Method)\\\")\\n\",\n",
    "    \"        print(\\\"Silakan lakukan gesture: tilt kanan, tilt kiri, netral\\\")\\n\",\n",
    "    \"        print(\\\"Press 'q' to stop testing\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        while frame_count < 100:  # Test for 100 frames\\n\",\n",
    "    \"            ret, frame = cap.read()\\n\",\n",
    "    \"            if not ret:\\n\",\n",
    "    \"                break\\n\",\n",
    "    \"                \\n\",\n",
    "    \"            # Flip frame horizontally (like Acit's app)\\n\",\n",
    "    \"            frame = cv2.flip(frame, 1)\\n\",\n",
    "    \"            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Process frame\\n\",\n",
    "    \"            process_start = time.time()\\n\",\n",
    "    \"            results = face_mesh.process(rgb_frame)\\n\",\n",
    "    \"            process_time = (time.time() - process_start) * 1000\\n\",\n",
    "    \"            processing_times.append(process_time)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Draw landmarks and analyze if face detected\\n\",\n",
    "    \"            if results.multi_face_landmarks:\\n\",\n",
    "    \"                head_detected_frames += 1\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                for face_landmarks in results.multi_face_landmarks:\\n\",\n",
    "    \"                    # Draw face mesh contours (like Acit's visualization)\\n\",\n",
    "    \"                    mp_drawing.draw_landmarks(\\n\",\n",
    "    \"                        frame,\\n\",\n",
    "    \"                        face_landmarks,\\n\",\n",
    "    \"                        mp_face_mesh.FACEMESH_CONTOURS,\\n\",\n",
    "    \"                        landmark_drawing_spec=None,\\n\",\n",
    "    \"                        connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1)\\n\",\n",
    "    \"                    )\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Calculate head pose using Acit's method\\n\",\n",
    "    \"                    head_pose = calculate_head_pose_acit_style(face_landmarks.landmark, frame.shape[:2])\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Draw head pose indicators (like Acit's app)\\n\",\n",
    "    \"                    nose_tip = head_pose['nose_tip']\\n\",\n",
    "    \"                    eye_center = head_pose['eye_center']\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    cv2.circle(frame, nose_tip, 5, (0, 0, 255), -1)\\n\",\n",
    "    \"                    cv2.circle(frame, eye_center, 3, (255, 0, 0), -1)\\n\",\n",
    "    \"                    cv2.line(frame, eye_center, nose_tip, (255, 255, 0), 2)\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Detect gestures\\n\",\n",
    "    \"                    gesture, roll_angle = detect_head_gestures_acit_style(head_pose)\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Display head tilt angle (like Acit's app)\\n\",\n",
    "    \"                    cv2.putText(frame, f\\\"Head Tilt: {roll_angle:.1f}°\\\", (10, 30), \\n\",\n",
    "    \"                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Display detected gesture\\n\",\n",
    "    \"                    if gesture:\\n\",\n",
    "    \"                        gesture_text = {\\n\",\n",
    "    \"                            \\\"tilt_right\\\": \\\"TILT RIGHT - NEXT SLIDE\\\",\\n\",\n",
    "    \"                            \\\"tilt_left\\\": \\\"TILT LEFT - PREVIOUS SLIDE\\\",\\n\",\n",
    "    \"                            \\\"triple_tilt_candidate\\\": \\\"STRONG TILT - TRIPLE TILT CANDIDATE\\\"\\n\",\n",
    "    \"                        }\\n\",\n",
    "    \"                        cv2.putText(frame, gesture_text.get(gesture, gesture), (10, 70), \\n\",\n",
    "    \"                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Store gesture data for accuracy analysis\\n\",\n",
    "    \"                    gesture_accuracy_data.append({\\n\",\n",
    "    \"                        'frame': frame_count,\\n\",\n",
    "    \"                        'roll_angle': roll_angle,\\n\",\n",
    "    \"                        'gesture_detected': gesture,\\n\",\n",
    "    \"                        'processing_time': process_time\\n\",\n",
    "    \"                    })\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Calculate FPS\\n\",\n",
    "    \"            frame_count += 1\\n\",\n",
    "    \"            if frame_count % 30 == 0:\\n\",\n",
    "    \"                elapsed_time = time.time() - start_time\\n\",\n",
    "    \"                fps = 30 / elapsed_time\\n\",\n",
    "    \"                fps_list.append(fps)\\n\",\n",
    "    \"                start_time = time.time()\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Display frame counter and detection status\\n\",\n",
    "    \"            cv2.putText(frame, f'Frame: {frame_count}/100', (10, frame.shape[0] - 60), \\n\",\n",
    "    \"                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\\n\",\n",
    "    \"            cv2.putText(frame, f'Head Detection: {head_detected_frames}/{frame_count}', \\n\",\n",
    "    \"                       (10, frame.shape[0] - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, \\n\",\n",
    "    \"                       (0, 255, 0) if results.multi_face_landmarks else (0, 0, 255), 2)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Show instructions\\n\",\n",
    "    \"            instructions = [\\n\",\n",
    "    \"                \\\"Tilt Right: Next slide (15°+)\\\",\\n\",\n",
    "    \"                \\\"Tilt Left: Previous slide (15°+)\\\", \\n\",\n",
    "    \"                \\\"Strong Tilt: Triple tilt candidate (20°+)\\\",\\n\",\n",
    "    \"                \\\"Keep head visible in frame\\\"\\n\",\n",
    "    \"            ]\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            for i, instruction in enumerate(instructions):\\n\",\n",
    "    \"                cv2.putText(frame, instruction, (10, 100 + i * 25), \\n\",\n",
    "    \"                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            cv2.imshow('Baseline Head Gesture Analysis (Acit\\\\'s Method)', frame)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            if cv2.waitKey(1) & 0xFF == ord('q'):\\n\",\n",
    "    \"                break\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    cap.release()\\n\",\n",
    "    \"    cv2.destroyAllWindows()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate performance metrics\\n\",\n",
    "    \"    avg_fps = np.mean(fps_list) if fps_list else 0\\n\",\n",
    "    \"    avg_processing_time = np.mean(processing_times)\\n\",\n",
    "    \"    detection_rate = head_detected_frames / frame_count if frame_count > 0 else 0\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Analyze gesture accuracy\\n\",\n",
    "    \"    gesture_df = pd.DataFrame(gesture_accuracy_data)\\n\",\n",
    "    \"    successful_gestures = len(gesture_df[gesture_df['gesture_detected'].notna()])\\n\",\n",
    "    \"    gesture_accuracy = successful_gestures / len(gesture_df) if len(gesture_df) > 0 else 0\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Store performance data\\n\",\n",
    "    \"    performance_data['method'].append('Head Gesture (Baseline - Acit\\\\'s Method)')\\n\",\n",
    "    \"    performance_data['fps'].append(avg_fps)\\n\",\n",
    "    \"    performance_data['processing_time_ms'].append(avg_processing_time)\\n\",\n",
    "    \"    performance_data['landmarks_count'].append(468)  # Face mesh landmarks\\n\",\n",
    "    \"    performance_data['detection_confidence'].append(detection_rate)\\n\",\n",
    "    \"    performance_data['accuracy_rate'].append(gesture_accuracy)\\n\",\n",
    "    \"    performance_data['use_case'].append('PowerPoint Control')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\n✅ Baseline Head Gesture Analysis completed!\\\")\\n\",\n",
    "    \"    print(f\\\"Average FPS: {avg_fps:.2f}\\\")\\n\",\n",
    "    \"    print(f\\\"Average Processing Time: {avg_processing_time:.2f}ms\\\")\\n\",\n",
    "    \"    print(f\\\"Head Detection Rate: {detection_rate:.2%}\\\")\\n\",\n",
    "    \"    print(f\\\"Gesture Detection Accuracy: {gesture_accuracy:.2%}\\\")\\n\",\n",
    "    \"    print(f\\\"Total Gestures Detected: {successful_gestures}/{len(gesture_df)}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        'avg_fps': avg_fps,\\n\",\n",
    "    \"        'avg_processing_time': avg_processing_time,\\n\",\n",
    "    \"        'detection_rate': detection_rate,\\n\",\n",
    "    \"        'gesture_accuracy': gesture_accuracy,\\n\",\n",
    "    \"        'landmarks_count': 468,\\n\",\n",
    "    \"        'gesture_data': gesture_df\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Run baseline test\\n\",\n",
    "    \"baseline_results = test_baseline_head_gesture_system()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 🏃‍♂️ 3. MediaPipe Pose - Beyond Head: Full Body Tracking\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Initialize MediaPipe Pose\\n\",\n",
    "    \"mp_pose = mp.solutions.pose\\n\",\n",
    "    \"\\n\",\n",
    "    \"def test_pose_body_gestures():\\n\",\n",
    "    \"    \\\"\\\"\\\"Test MediaPipe Pose for full body gesture control beyond head\\\"\\\"\\\"\\n\",\n",
    "    \"    cap = cv2.VideoCapture(0)\\n\",\n",
    "    \"    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\\n\",\n",
    "    \"    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    with mp_pose.Pose(\\n\",\n",
    "    \"        min_detection_confidence=0.5,\\n\",\n",
    "    \"        min_tracking_confidence=0.5,\\n\",\n",
    "    \"        model_complexity=1  # 0=Lite, 1=Full, 2=Heavy\\n\",\n",
    "    \"    ) as pose:\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        frame_count = 0\\n\",\n",
    "    \"        start_time = time.time()\\n\",\n",
    "    \"        fps_list = []\\n\",\n",
    "    \"        processing_times = []\\n\",\n",
    "    \"        pose_detected_frames = 0\\n\",\n",
    "    \"        gesture_data = []\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(\\\"🏃‍♂️ Testing Body Pose Gestures (Beyond Head Tracking)\\\")\\n\",\n",
    "    \"        print(\\\"Try: Raise hand, point left/right, arms crossed, standing/sitting\\\")\\n\",\n",
    "    \"        print(\\\"Press 'q' to stop\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        while frame_count < 100:\\n\",\n",
    "    \"            ret, frame = cap.read()\\n\",\n",
    "    \"            if not ret:\\n\",\n",
    "    \"                break\\n\",\n",
    "    \"                \\n\",\n",
    "    \"            frame = cv2.flip(frame, 1)\\n\",\n",
    "    \"            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Process frame\\n\",\n",
    "    \"            process_start = time.time()\\n\",\n",
    "    \"            results = pose.process(rgb_frame)\\n\",\n",
    "    \"            process_time = (time.time() - process_start) * 1000\\n\",\n",
    "    \"            processing_times.append(process_time)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Draw pose landmarks\\n\",\n",
    "    \"            if results.pose_landmarks:\\n\",\n",
    "    \"                pose_detected_frames += 1\\n\",\n",
    "    \"                mp_drawing.draw_landmarks(\\n\",\n",
    "    \"                    frame,\\n\",\n",
    "    \"                    results.pose_landmarks,\\n\",\n",
    "    \"                    mp_pose.POSE_CONNECTIONS,\\n\",\n",
    "    \"                    mp_drawing_styles.get_default_pose_landmarks_style()\\n\",\n",
    "    \"                )\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Extract key body pose features\\n\",\n",
    "    \"                landmarks = results.pose_landmarks.landmark\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Body measurements and gesture detection\\n\",\n",
    "    \"                left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]\\n\",\n",
    "    \"                right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]\\n\",\n",
    "    \"                left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST]\\n\",\n",
    "    \"                right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST]\\n\",\n",
    "    \"                left_elbow = landmarks[mp_pose.PoseLandmark.LEFT_ELBOW]\\n\",\n",
    "    \"                right_elbow = landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW]\\n\",\n",
    "    \"                nose = landmarks[mp_pose.PoseLandmark.NOSE]\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Calculate body metrics\\n\",\n",
    "    \"                shoulder_width = abs(left_shoulder.x - right_shoulder.x)\\n\",\n",
    "    \"                shoulder_y_avg = (left_shoulder.y + right_shoulder.y) / 2\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Hand position analysis (beyond head gestures)\\n\",\n",
    "    \"                left_hand_raised = left_wrist.y < left_shoulder.y - 0.1\\n\",\n",
    "    \"                right_hand_raised = right_wrist.y < right_shoulder.y - 0.1\\n\",\n",
    "    \"                both_hands_raised = left_hand_raised and right_hand_raised\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Pointing gestures\\n\",\n",
    "    \"                pointing_left = right_wrist.x < right_shoulder.x - 0.2 and right_wrist.y < right_shoulder.y\\n\",\n",
    "    \"                pointing_right = left_wrist.x > left_shoulder.x + 0.2 and left_wrist.y < left_shoulder.y\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Arms crossed detection\\n\",\n",
    "    \"                arms_crossed = (left_wrist.x > right_shoulder.x - 0.1 and \\n\",\n",
    "    \"                               right_wrist.x < left_shoulder.x + 0.1 and\\n\",\n",
    "    \"                               abs(left_wrist.y - right_wrist.y) < 0.15)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Body leaning detection\\n\",\n",
    "    \"                body_lean_left = (left_shoulder.y > right_shoulder.y + 0.05)\\n\",\n",
    "    \"                body_lean_right = (right_shoulder.y > left_shoulder.y + 0.05)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Gesture classification for presentation control\\n\",\n",
    "    \"                detected_gestures = []\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                if both_hands_raised:\\n\",\n",
    "    \"                    detected_gestures.append(\\\"BOTH_HANDS_UP\\\")\\n\",\n",
    "    \"                elif left_hand_raised and not right_hand_raised:\\n\",\n",
    "    \"                    detected_gestures.append(\\\"LEFT_HAND_UP\\\")\\n\",\n",
    "    \"                elif right_hand_raised and not left_hand_raised:\\n\",\n",
    "    \"                    detected_gestures.append(\\\"RIGHT_HAND_UP\\\")\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                if pointing_left:\\n\",\n",
    "    \"                    detected_gestures.append(\\\"POINTING_LEFT\\\")\\n\",\n",
    "    \"                elif pointing_right:\\n\",\n",
    "    \"                    detected_gestures.append(\\\"POINTING_RIGHT\\\")\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                if arms_crossed:\\n\",\n",
    "    \"                    detected_gestures.append(\\\"ARMS_CROSSED\\\")\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                if body_lean_left:\\n\",\n",
    "    \"                    detected_gestures.append(\\\"LEAN_LEFT\\\")\\n\",\n",
    "    \"                elif body_lean_right:\\n\",\n",
    "    \"                    detected_gestures.append(\\\"LEAN_RIGHT\\\")\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Display body metrics\\n\",\n",
    "    \"                cv2.putText(frame, f'Shoulder Width: {shoulder_width:.3f}', \\n\",\n",
    "    \"                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Display detected gestures\\n\",\n",
    "    \"                if detected_gestures:\\n\",\n",
    "    \"                    gesture_text = \\\" | \\\".join(detected_gestures)\\n\",\n",
    "    \"                    cv2.putText(frame, f'Gestures: {gesture_text}', \\n\",\n",
    "    \"                               (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Potential presentation control mapping\\n\",\n",
    "    \"                control_suggestion = \\\"\\\"\\n\",\n",
    "    \"                if \\\"RIGHT_HAND_UP\\\" in detected_gestures:\\n\",\n",
    "    \"                    control_suggestion = \\\"→ NEXT SLIDE\\\"\\n\",\n",
    "    \"                elif \\\"LEFT_HAND_UP\\\" in detected_gestures:\\n\",\n",
    "    \"                    control_suggestion = \\\"← PREVIOUS SLIDE\\\"\\n\",\n",
    "    \"                elif \\\"BOTH_HANDS_UP\\\" in detected_gestures:\\n\",\n",
    "    \"                    control_suggestion = \\\"↑ START/STOP PRESENTATION\\\"\\n\",\n",
    "    \"                elif \\\"ARMS_CROSSED\\\" in detected_gestures:\\n\",\n",
    "    \"                    control_suggestion = \\\"✕ EXIT PRESENTATION\\\"\\n\",\n",
    "    \"                elif \\\"POINTING_LEFT\\\" in detected_gestures:\\n\",\n",
    "    \"                    control_suggestion = \\\"← JUMP TO BEGINNING\\\"\\n\",\n",
    "    \"                elif \\\"POINTING_RIGHT\\\" in detected_gestures:\\n\",\n",
    "    \"                    control_suggestion = \\\"→ JUMP TO END\\\"\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                if control_suggestion:\\n\",\n",
    "    \"                    cv2.putText(frame, f'Control: {control_suggestion}', \\n\",\n",
    "    \"                               (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Store gesture data\\n\",\n",
    "    \"                gesture_data.append({\\n\",\n",
    "    \"                    'frame': frame_count,\\n\",\n",
    "    \"                    'shoulder_width': shoulder_width,\\n\",\n",
    "    \"                    'left_hand_raised': left_hand_raised,\\n\",\n",
    "    \"                    'right_hand_raised': right_hand_raised,\\n\",\n",
    "    \"                    'gestures': detected_gestures,\\n\",\n",
    "    \"                    'control_suggestion': control_suggestion\\n\",\n",
    "    \"                })\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Calculate FPS\\n\",\n",
    "    \"            frame_count += 1\\n\",\n",
    "    \"            if frame_count % 30 == 0:\\n\",\n",
    "    \"                elapsed_time = time.time() - start_time\\n\",\n",
    "    \"                fps = 30 / elapsed_time\\n\",\n",
    "    \"                fps_list.append(fps)\\n\",\n",
    "    \"                start_time = time.time()\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Display info\\n\",\n",
    "    \"            cv2.putText(frame, f'Frame: {frame_count}/100', (10, frame.shape[0] - 60), \\n\",\n",
    "    \"                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\\n\",\n",
    "    \"            cv2.putText(frame, f'Pose Detection: {pose_detected_frames}/{frame_count}', \\n\",\n",
    "    \"                       (10, frame.shape[0] - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, \\n\",\n",
    "    \"                       (0, 255, 0) if results.pose_landmarks else (0, 0, 255), 2)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            cv2.imshow('Body Pose Gestures (Beyond Head)', frame)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            if cv2.waitKey(1) & 0xFF == ord('q'):\\n\",\n",
    "    \"                break\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    cap.release()\\n\",\n",
    "    \"    cv2.destroyAllWindows()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Store performance data\\n\",\n",
    "    \"    avg_fps = np.mean(fps_list) if fps_list else 0\\n\",\n",
    "    \"    avg_processing_time = np.mean(processing_times)\\n\",\n",
    "    \"    detection_rate = pose_detected_frames / frame_count if frame_count > 0 else 0\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Analyze gesture variety\\n\",\n",
    "    \"    gesture_df = pd.DataFrame(gesture_data)\\n\",\n",
    "    \"    unique_gestures = set()\\n\",\n",
    "    \"    for gestures_list in gesture_df['gestures']:\\n\",\n",
    "    \"        unique_gestures.update(gestures_list)\\n\",\n",
    "    \"    gesture_variety = len(unique_gestures)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    performance_data['method'].append('Body Pose Gestures')\\n\",\n",
    "    \"    performance_data['fps'].append(avg_fps)\\n\",\n",
    "    \"    performance_data['processing_time_ms'].append(avg_processing_time)\\n\",\n",
    "    \"    performance_data['landmarks_count'].append(33)  # Pose landmarks\\n\",\n",
    "    \"    performance_data['detection_confidence'].append(detection_rate)\\n\",\n",
    "    \"    performance_data['accuracy_rate'].append(gesture_variety / 10)  # Normalized variety score\\n\",\n",
    "    \"    performance_data['use_case'].append('Body Control & Fitness')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\n✅ Body Pose Gesture Analysis completed!\\\")\\n\",\n",
    "    \"    print(f\\\"Average FPS: {avg_fps:.2f}\\\")\\n\",\n",
    "    \"    print(f\\\"Average Processing Time: {avg_processing_time:.2f}ms\\\")\\n\",\n",
    "    \"    print(f\\\"Pose Detection Rate: {detection_rate:.2%}\\\")\\n\",\n",
    "    \"    print(f\\\"Unique Gestures Detected: {gesture_variety}\\\")\\n\",\n",
    "    \"    print(f\\\"Detected gesture types: {list(unique_gestures)}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        'avg_fps': avg_fps,\\n\",\n",
    "    \"        'avg_processing_time': avg_processing_time,\\n\",\n",
    "    \"        'detection_rate': detection_rate,\\n\",\n",
    "    \"        'gesture_variety': gesture_variety,\\n\",\n",
    "    \"        'landmarks_count': 33,\\n\",\n",
    "    \"        'gesture_data': gesture_df\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Run pose detection test\\n\",\n",
    "    \"pose_results = test_pose_body_gestures()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 🎭 4. Enhanced Face Features - Beyond Basic Head Tilt\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def test_enhanced_face_features_beyond_head():\\n\",\n",
    "    \"    \\\"\\\"\\\"Test advanced face features beyond basic head tilt detection\\\"\\\"\\\"\\n\",\n",
    "    \"    cap = cv2.VideoCapture(0)\\n\",\n",
    "    \"    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\\n\",\n",
    "    \"    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    with mp_face_mesh.FaceMesh(\\n\",\n",
    "    \"        max_num_faces=2,  # Multiple faces\\n\",\n",
    "    \"        refine_landmarks=True,\\n\",\n",
    "    \"        min_detection_confidence=0.7,\\n\",\n",
    "    \"        min_tracking_confidence=0.7\\n\",\n",
    "    \"    ) as face_mesh:\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        frame_count = 0\\n\",\n",
    "    \"        start_time = time.time()\\n\",\n",
    "    \"        fps_list = []\\n\",\n",
    "    \"        processing_times = []\\n\",\n",
    "    \"        facial_expression_data = []\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(\\\"🎭 Testing Enhanced Face Features (Beyond Head Tilt)\\\")\\n\",\n",
    "    \"        print(\\\"Try: Blink, smile, open mouth, raise eyebrows, look around\\\")\\n\",\n",
    "    \"        print(\\\"Press 'q' to stop\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        while frame_count < 100:\\n\",\n",
    "    \"            ret, frame = cap.read()\\n\",\n",
    "    \"            if not ret:\\n\",\n",
    "    \"                break\\n\",\n",
    "    \"                \\n\",\n",
    "    \"            frame = cv2.flip(frame, 1)\\n\",\n",
    "    \"            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\\n\",\n",
    "    \"            h, w, _ = frame.shape\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Process frame\\n\",\n",
    "    \"            process_start = time.time()\\n\",\n",
    "    \"            results = face_mesh.process(rgb_frame)\\n\",\n",
    "    \"            process_time = (time.time() - process_start) * 1000\\n\",\n",
    "    \"            processing_times.append(process_time)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            if results.multi_face_landmarks:\\n\",\n",
    "    \"                for idx, face_landmarks in enumerate(results.multi_face_landmarks):\\n\",\n",
    "    \"                    # Draw refined landmarks\\n\",\n",
    "    \"                    mp_drawing.draw_landmarks(\\n\",\n",
    "    \"                        frame,\\n\",\n",
    "    \"                        face_landmarks,\\n\",\n",
    "    \"                        mp_face_mesh.FACEMESH_TESSELATION,\\n\",\n",
    "    \"                        None,\\n\",\n",
    "    \"                        mp_drawing_styles.get_default_face_mesh_tesselation_style()\\n\",\n",
    "    \"                    )\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Extract advanced facial features (beyond head tilt)\\n\",\n",
    "    \"                    landmarks = face_landmarks.landmark\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Eye analysis (blink detection)\\n\",\n",
    "    \"                    left_eye_top = landmarks[159]\\n\",\n",
    "    \"                    left_eye_bottom = landmarks[145]\\n\",\n",
    "    \"                    right_eye_top = landmarks[386]\\n\",\n",
    "    \"                    right_eye_bottom = landmarks[374]\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    left_eye_height = abs(left_eye_top.y - left_eye_bottom.y)\\n\",\n",
    "    \"                    right_eye_height = abs(right_eye_top.y - right_eye_bottom.y)\\n\",\n",
    "    \"                    avg_eye_height = (left_eye_height + right_eye_height) / 2\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Eyebrow analysis (surprise detection)\\n\",\n",
    "    \"                    left_eyebrow = landmarks[70]\\n\",\n",
    "    \"                    right_eyebrow = landmarks[300]\\n\",\n",
    "    \"                    eyebrow_height = (left_eyebrow.y + right_eyebrow.y) / 2\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Mouth analysis (smile, open mouth detection)\\n\",\n",
    "    \"                    mouth_left = landmarks[61]\\n\",\n",
    "    \"                    mouth_right = landmarks[291]\\n\",\n",
    "    \"                    mouth_top = landmarks[13]\\n\",\n",
    "    \"                    mouth_bottom = landmarks[14]\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    mouth_width = abs(mouth_left.x - mouth_right.x)\\n\",\n",
    "    \"                    mouth_height = abs(mouth_top.y - mouth_bottom.y)\\n\",\n",
    "    \"                    mouth_ratio = mouth_width / mouth_height if mouth_height > 0 else 0\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Eye gaze direction (beyond head tilt)\\n\",\n",
    "    \"                    left_eye_center = landmarks[468]\\n\",\n",
    "    \"                    right_eye_center = landmarks[473]\\n\",\n",
    "    \"                    nose_tip = landmarks[1]\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Calculate gaze direction\\n\",\n",
    "    \"                    eye_center_x = (left_eye_center.x + right_eye_center.x) / 2\\n\",\n",
    "    \"                    gaze_offset = nose_tip.x - eye_center_x\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Advanced expression detection\\n\",\n",
    "    \"                    is_blinking = avg_eye_height < 0.008\\n\",\n",
    "    \"                    is_smiling = mouth_ratio > 3.2\\n\",\n",
    "    \"                    is_mouth_open = mouth_height > 0.02\\n\",\n",
    "    \"                    is_surprised = eyebrow_height < 0.3  # Higher eyebrows\\n\",\n",
    "    \"                    gaze_direction = \\\"CENTER\\\"\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    if gaze_offset > 0.02:\\n\",\n",
    "    \"                        gaze_direction = \\\"RIGHT\\\"\\n\",\n",
    "    \"                    elif gaze_offset < -0.02:\\n\",\n",
    "    \"                        gaze_direction = \\\"LEFT\\\"\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Advanced gesture classification for presentation control\\n\",\n",
    "    \"                    advanced_gestures = []\\n\",\n",
    "    \"                    control_commands = []\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    if is_blinking:\\n\",\n",
    "    \"                        advanced_gestures.append(\\\"BLINK\\\")\\n\",\n",
    "    \"                        control_commands.append(\\\"→ CLICK/SELECT\\\")\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    if is_smiling:\\n\",\n",
    "    \"                        advanced_gestures.append(\\\"SMILE\\\")\\n\",\n",
    "    \"                        control_commands.append(\\\"→ POSITIVE FEEDBACK\\\")\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    if is_mouth_open:\\n\",\n",
    "    \"                        advanced_gestures.append(\\\"MOUTH_OPEN\\\")\\n\",\n",
    "    \"                        control_commands.append(\\\"→ VOICE COMMAND READY\\\")\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    if is_surprised:\\n\",\n",
    "    \"                        advanced_gestures.append(\\\"EYEBROWS_UP\\\")\\n\",\n",
    "    \"                        control_commands.append(\\\"→ ATTENTION/HIGHLIGHT\\\")\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    if gaze_direction != \\\"CENTER\\\":\\n\",\n",
    "    \"                        advanced_gestures.append(f\\\"GAZE_{gaze_direction}\\\")\\n\",\n",
    "    \"                        control_commands.append(f\\\"→ LOOK {gaze_direction}\\\")\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Display analysis results\\n\",\n",
    "    \"                    y_offset = 30 + (idx * 200)\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    cv2.putText(frame, f'Face {idx+1} Advanced Features:', (10, y_offset), \\n\",\n",
    "    \"                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    cv2.putText(frame, f'Eye: {avg_eye_height:.4f} | Mouth: {mouth_ratio:.2f}', \\n\",\n",
    "    \"                               (10, y_offset + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    cv2.putText(frame, f'Gaze: {gaze_direction} | Eyebrow: {eyebrow_height:.3f}', \\n\",\n",
    "    \"                               (10, y_offset + 45), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    if advanced_gestures:\\n\",\n",
    "    \"                        gesture_text = \\\" | \\\".join(advanced_gestures)\\n\",\n",
    "    \"                        cv2.putText(frame, f'Expressions: {gesture_text}', \\n\",\n",
    "    \"                                   (10, y_offset + 70), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 2)\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    if control_commands:\\n\",\n",
    "    \"                        command_text = \\\" \\\".join(control_commands[:2])  # Show first 2 commands\\n\",\n",
    "    \"                        cv2.putText(frame, f'Controls: {command_text}', \\n\",\n",
    "    \"                                   (10, y_offset + 95), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\\n\",\n",
    "    \"                    \\n\",\n",
    "    \"                    # Store facial expression data\\n\",\n",
    "    \"                    facial_expression_data.append({\\n\",\n",
    "    \"                        'frame': frame_count,\\n\",\n",
    "    \"                        'face_id': idx,\\n\",\n",
    "    \"                        'eye_height': avg_eye_height,\\n\",\n",
    "    \"                        'mouth_ratio': mouth_ratio,\\n\",\n",
    "    \"                        'mouth_height': mouth_height,\\n\",\n",
    "    \"                        'eyebrow_height': eyebrow_height,\\n\",\n",
    "    \"                        'gaze_direction': gaze_direction,\\n\",\n",
    "    \"                        'is_blinking': is_blinking,\\n\",\n",
    "    \"                        'is_smiling': is_smiling,\\n\",\n",
    "    \"                        'is_mouth_open': is_mouth_open,\\n\",\n",
    "    \"                        'is_surprised': is_surprised,\\n\",\n",
    "    \"                        'advanced_gestures': advanced_gestures,\\n\",\n",
    "    \"                        'control_commands': control_commands\\n\",\n",
    "    \"                    })\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Calculate FPS\\n\",\n",
    "    \"            frame_count += 1\\n\",\n",
    "    \"            if frame_count % 30 == 0:\\n\",\n",
    "    \"                elapsed_time = time.time() - start_time\\n\",\n",
    "    \"                fps = 30 / elapsed_time\\n\",\n",
    "    \"                fps_list.append(fps)\\n\",\n",
    "    \"                start_time = time.time()\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            cv2.putText(frame, f'Frame: {frame_count}/100', (10, frame.shape[0] - 60), \\n\",\n",
    "    \"                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\\n\",\n",
    "    \"            cv2.putText(frame, f'Faces: {len(results.multi_face_landmarks) if results.multi_face_landmarks else 0}', \\n\",\n",
    "    \"                       (10, frame.shape[0] - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            cv2.imshow('Enhanced Face Features (Beyond Head Tilt)', frame)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            if cv2.waitKey(1) & 0xFF == ord('q'):\\n\",\n",
    "    \"                break\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    cap.release()\\n\",\n",
    "    \"    cv2.destroyAllWindows()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate performance metrics\\n\",\n",
    "    \"    avg_fps = np.mean(fps_list) if fps_list else 0\\n\",\n",
    "    \"    avg_processing_time = np.mean(processing_times)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Analyze expression variety and accuracy\\n\",\n",
    "    \"    expression_df = pd.DataFrame(facial_expression_data)\\n\",\n",
    "    \"    unique_expressions = set()\\n\",\n",
    "    \"    for expr_list in expression_df['advanced_gestures']:\\n\",\n",
    "    \"        unique_expressions.update(expr_list)\\n\",\n",
    "    \"    expression_variety = len(unique_expressions)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate expression detection accuracy\\n\",\n",
    "    \"    total_detections = len(expression_df[expression_df['advanced_gestures'].apply(len) > 0])\\n\",\n",
    "    \"    expression_accuracy = total_detections / len(expression_df) if len(expression_df) > 0 else 0\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    performance_data['method'].append('Enhanced Face Features')\\n\",\n",
    "    \"    performance_data['fps'].append(avg_fps)\\n\",\n",
    "    \"    performance_data['processing_time_ms'].append(avg_processing_time)\\n\",\n",
    "    \"    performance_data['landmarks_count'].append(468)\\n\",\n",
    "    \"    performance_data['detection_confidence'].append(0.7)\\n\",\n",
    "    \"    performance_data['accuracy_rate'].append(expression_accuracy)\\n\",\n",
    "    \"    performance_data['use_case'].append('Emotion AI & Accessibility')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\n✅ Enhanced Face Features Analysis completed!\\\")\\n\",\n",
    "    \"    print(f\\\"Average FPS: {avg_fps:.2f}\\\")\\n\",\n",
    "    \"    print(f\\\"Average Processing Time: {avg_processing_time:.2f}ms\\\")\\n\",\n",
    "    \"    print(f\\\"Expression Detection Accuracy: {expression_accuracy:.2%}\\\")\\n\",\n",
    "    \"    print(f\\\"Unique Expressions Detected: {expression_variety}\\\")\\n\",\n",
    "    \"    print(f\\\"Expression types: {list(unique_expressions)}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        'avg_fps': avg_fps,\\n\",\n",
    "    \"        'avg_processing_time': avg_processing_time,\\n\",\n",
    "    \"        'expression_accuracy': expression_accuracy,\\n\",\n",
    "    \"        'expression_variety': expression_variety,\\n\",\n",
    "    \"        'landmarks_count': 468,\\n\",\n",
    "    \"        'expression_data': expression_df\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Run enhanced face features test\\n\",\n",
    "    \"enhanced_face_results = test_enhanced_face_features_beyond_head()\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e9305f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (233752286.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m```json\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
